# Introduction
This repo serves as an introductory resource and guide for those interested in deploying LLM solutions on AMD platforms. It covers a high level discussion on the foundation of LLMs and includes practical implementations and slides users can follow along. Source code can also be run on Nvidia platforms without modification.

# Target Audience
- Programmers familiar with Python
- Computer Science or Electrical Engineering students interested in building LLM solutions on ROCm or AMD AI PCs
- Anyone looking to build their first LLM project

# Included Materials
- Setting up your AMD environment (ROCm or AI PC/NPU) [**PDF**]
- Deploying a LLM server using Ollama and LM Studio [**PDF**]
- LLM inferencing using transformers [**inference_transformers.py**]
- LLM inferencing using API (Ollama and LM Studio) [**inference_ollama.py**]
- Fine tuning your model with custom dataset(s) to perform function calling [**gemma_function_calling_finetune.py**]
- Function calling using Open-Meteo as an example [**function_calling_transformers.py**, **function_calling_ollama.py**]
- End-to-end RAG example using LM Studio to host query generator [**rag_query_ollama.py**] 
- Lecture slides with high level discussions on LLM fundamentals [**PDF**]
